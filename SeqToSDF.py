#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Converts CSV table to a Structure Data File (SDF)
input - json config
@author: SmirnygaTotoshka, f-normies
@version 2.0
"""

import argparse
import json
import os
import traceback
import multiprocessing
import time
import sys
import pandas as pd
from rdkit import Chem
from concurrent.futures import ProcessPoolExecutor
import logging

# Оптимизация - используем множества для быстрой проверки символов
ALPHABETS = {
    "protein": set("ACDEFGHIKLMNPQRSTVWY"),
    "DNA": set("ATGC"),
    "RNA": set("AUGC")
}

# Кэшируем SMARTS-шаблоны для однократного создания
CARBOXYL = Chem.MolFromSmarts('C(=O)O')
LYSINE = Chem.MolFromSmarts('C(CCN)C[C@@H](C(=O))')
IMYDAZOLINE = Chem.MolFromSmarts('c1cnc[nH]1')
GUANIDINE = Chem.MolFromSmarts('NC(N)=N')

def setup_logging(output_dir, filename, thread_id):
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, f"{filename}_thread_{thread_id}_log.txt")
    
    logger = logging.getLogger(f"thread_{thread_id}")
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    return logger

def isCorrectSequence(sequence, alphabet):
    seq = sequence.strip()
    alphabet_set = ALPHABETS[alphabet]
    return all(c in alphabet_set for c in seq)

def chargePeptide(mol):
    mol.GetAtomWithIdx(0).SetFormalCharge(1)  # charge N end of peptide

    carboxyl_groups_pos = list(mol.GetSubstructMatches(CARBOXYL))
    if carboxyl_groups_pos:
        for g in carboxyl_groups_pos:
            mol.GetAtomWithIdx(g[2]).SetFormalCharge(-1)

    lysine_groups_pos = list(mol.GetSubstructMatches(LYSINE))
    if lysine_groups_pos:
        for g in lysine_groups_pos:
            mol.GetAtomWithIdx(g[3]).SetFormalCharge(1)

    imydazoline_groups_pos = list(mol.GetSubstructMatches(IMYDAZOLINE))
    if imydazoline_groups_pos:
        for g in imydazoline_groups_pos:
            mol.GetAtomWithIdx(g[4]).SetFormalCharge(1)

    guanidine_groups_pos = list(mol.GetSubstructMatches(GUANIDINE))
    if guanidine_groups_pos:
        for g in guanidine_groups_pos:
            mol.GetAtomWithIdx(g[3]).SetFormalCharge(1)

def write(proc, partition, output, sequence_column, isCharged, alphabet, output_filename):
    logger = setup_logging(output, output_filename, proc)
    out = os.path.join(output, f"{output_filename}_thread_{proc}.sdf")
    fail = os.path.join(output, f"{output_filename}_thread_{proc}_failed.txt")
    
    try:
        with open(out, "w", encoding="utf-8") as o, open(fail, "w", encoding="utf-8") as f:
            logger.info(f"Start thread #{proc}")
            
            for i in partition.index:
                try:
                    logger.info(f"Convert record #{i}")
                    
                    if isCorrectSequence(partition.loc[i, sequence_column], alphabet):
                        flavor = {"protein": 0, "DNA": 2, "RNA": 6}.get(alphabet, 0)
                        
                        molecule = Chem.MolFromSequence(partition.loc[i, sequence_column], flavor=flavor)
                        molecule.SetProp('_Name', partition.loc[i, sequence_column])
                        molecule.SetProp('MolFileInfo', "Generated by the modified script of A. Smirnov, Dept. Bioinformatics, Pirogov RNRMU.")
                        
                        if isCharged and alphabet == "protein":
                            chargePeptide(molecule)

                        o.write(Chem.MolToMolBlock(molecule, forceV3000=True) + "\n")
                        
                        for c in partition.columns:
                            if c != sequence_column:
                                o.write(f">  <{c}>\n")
                                o.write(f"{partition.loc[i, c]}\n\n")
                        
                        o.write("$$$$\n")
                        logger.info("SUCCESS!")
                    else:
                        logger.info("FAILED!")
                        f.write(f"{i}\t{partition.loc[i, sequence_column]}\n")
                
                except Exception as er:
                    logger.error(f"Exception on record #{i}\t{partition.loc[i, sequence_column]}")
                    logger.error(str(er))
                    f.write(f"{i}\t{partition.loc[i, sequence_column]}\n")
                    traceback.print_exc()
    
    except Exception as e:
        logger.error(f"Something went wrong in thread #{proc}")
        logger.error(str(e))
        traceback.print_exc()
        sys.exit(1)

def merge_files(output, output_filename, number_threads, delete_tmp=True):
    buffer_size = 1024 * 1024  # 1MB буфер
    
    sdf = os.path.join(output, f"{output_filename}.sdf")
    total_log = os.path.join(output, f"{output_filename}_log.txt")
    total_fail = os.path.join(output, f"{output_filename}_failed.txt")
    
    with open(sdf, "w", encoding="utf-8") as final, \
         open(total_log, "w", encoding="utf-8") as log, \
         open(total_fail, "w", encoding="utf-8") as failed:
        
        for i in range(number_threads):
            out_t = os.path.join(output, f"{output_filename}_thread_{i}.sdf")
            log_t = os.path.join(output, f"{output_filename}_thread_{i}_log.txt")
            fail_t = os.path.join(output, f"{output_filename}_thread_{i}_failed.txt")
            
            # Копируем содержимое SDF файла
            if os.path.exists(out_t):
                with open(out_t, "r", encoding="utf-8") as o:
                    while True:
                        chunk = o.read(buffer_size)
                        if not chunk:
                            break
                        final.write(chunk)
            
            # Копируем содержимое лог-файла
            if os.path.exists(log_t):
                with open(log_t, "r", encoding="utf-8") as l:
                    while True:
                        chunk = l.read(buffer_size)
                        if not chunk:
                            break
                        log.write(chunk)
            
            # Копируем содержимое файла с ошибками
            if os.path.exists(fail_t):
                with open(fail_t, "r", encoding="utf-8") as f:
                    while True:
                        chunk = f.read(buffer_size)
                        if not chunk:
                            break
                        failed.write(chunk)
            
            # Удаляем временные файлы
            if delete_tmp:
                for file_path in [out_t, log_t, fail_t]:
                    if os.path.exists(file_path):
                        os.remove(file_path)

def validate_config(parameters):
    required = ["input", "output", "column"]
    
    for param in required:
        if param not in parameters:
            raise ValueError(f"Missing required parameter: {param}")
    
    if not os.path.exists(parameters["input"]) or not os.path.isfile(parameters["input"]):
        raise FileNotFoundError(f"Input file doesn't exist or it isn't a file: {parameters['input']}")
    
    if not os.path.exists(parameters["output"]):
        raise FileNotFoundError(f"Output directory doesn't exist: {parameters['output']}")
    
    # Проверяем опциональные параметры и устанавливаем значения по умолчанию
    parameters.setdefault("charged", True)
    parameters.setdefault("alphabet", "protein")
    parameters.setdefault("threads", 1)
    parameters.setdefault("separator", ";")
    parameters.setdefault("delete_tmp", True)
    
    if "filename" not in parameters:
        parameters["filename"] = os.path.splitext(os.path.basename(parameters["input"]))[0]
    
    if parameters["alphabet"] not in ALPHABETS:
        raise ValueError(f"Invalid alphabet. Allow only {list(ALPHABETS.keys())}")
    
    max_threads = 2 * multiprocessing.cpu_count()
    if parameters["threads"] < 1 or parameters["threads"] > max_threads:
        raise ValueError(f"Invalid thread count. Please use from 1 to {max_threads} threads.")
    
    return parameters

def main():
    start_time = time.time()
    
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Path to config file.")
    args = parser.parse_args()
    
    if not os.path.exists(args.config):
        print("Config doesn't exist.")
        sys.exit(1)
    
    try:
        with open(args.config, "r") as cfg:
            parameters = json.loads(''.join(cfg.readlines()))
        
        parameters = validate_config(parameters)
        
        input_file = parameters["input"]
        output_dir = parameters["output"]
        sequence_column = parameters["column"]
        isCharged = parameters["charged"]
        alphabet = parameters["alphabet"]
        number_threads = parameters["threads"]
        separator = parameters["separator"]
        output_filename = parameters["filename"]
        delete_tmp = parameters["delete_tmp"]
        
        # Читаем таблицу и проверяем наличие нужного столбца
        table = pd.read_csv(input_file, sep=separator, header=0, na_values="", keep_default_na=False)
        
        if sequence_column not in table.columns:
            raise ValueError(f"Table doesn't contain column: {sequence_column}")
        
        # Распределяем работу между потоками
        total = len(table.index)
        number_threads = min(number_threads, total)
        size_part = total // number_threads
        size_last_part = size_part + (total - size_part * number_threads)
        
        with ProcessPoolExecutor(max_workers=number_threads) as executor:
            futures = []
            
            for proc, start in zip(range(number_threads), range(0, total, size_part)):
                if proc == number_threads - 1:
                    partition = table[start:start + size_last_part]
                else:
                    partition = table[start:start + size_part]
                
                future = executor.submit(
                    write, proc, partition, output_dir, sequence_column, 
                    isCharged, alphabet, output_filename
                )
                futures.append(future)
            
            # Ждем завершения всех задач
            for future in futures:
                future.result()
        
        # Объединяем результаты всех потоков
        merge_files(output_dir, output_filename, number_threads, delete_tmp)
        
        print("Success")
    
    except Exception as e:
        print("Something went wrong")
        traceback.print_exc(file=sys.stdout)
        sys.exit(1)
    
    finally:
        end_time = time.time()
        print(f"--- {end_time - start_time:.2f} seconds ---")

if __name__ == '__main__':
    main()
